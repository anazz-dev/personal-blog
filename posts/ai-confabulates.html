<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ahmad M. Nazzal — AI Confabulates</title>
    <link rel="stylesheet" href="../styles.css">
    <!-- Google Fonts - Serif font similar to Tiempos Text -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;1,300;1,400&family=PT+Serif:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
    <link rel="icon" href="../favicon.ico" type="image/x-icon">
</head>
<body>
    <div class="container">
        <header>
            <h1><a href="../index.html">Ahmad M. Nazzal</a></h1>
            <div class="theme-toggle">
                <label for="theme-switch" class="theme-switch">
                    <input type="checkbox" id="theme-switch">
                    <span class="slider round"></span>
                </label>
            </div>
        </header>

        <div class="contents">
            <h2>Contents</h2>
            <ul>
                <li><a href="#what-are-hallucinations">What exactly are hallucinations?</a></li>
                <li><a href="#clinical-definition">What is the clinical definition of confabulations?</a></li>
                <li><a href="#about-ai">What About AI?</a></li>
                <li><a href="#call-it">Call It What It Is</a></li>
            </ul>
        </div>

        <main>
            <article class="post">
                <h1 class="post-title">AI Confabulates</h1>
                <p class="post-date">May 21, 2025</p>

                <div class="post-content">
                    <p>
                      Since the introduction of ChatGPT and large language models, I will refer to here as AI, they have made a grand entry into nearly everyone's life.
                      You have probably noticed they come with a disclaimer: they make mistakes. For some reason, these mistakes have been labeled "hallucinations." As usual, the field of AI naively borrows from neuroscience and psychology. In fact, calling these mistakes "hallucinations" is more poetic than precise; the proper term is confabulations.
                    </p>

                    <h2 id="what-are-hallucinations">What exactly are hallucinations?</h2>
                    <p>
                      According to the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition, Text Revision (DSM-5-TR, 2022), hallucinations are defined as perceptual experiences that occur in the absence of an external stimulus and are experienced as real.
                      People with conditions like schizophrenia may hear voices that no one else hears. Others with Parkinson's disease or certain types of dementia might see things that aren't really present. Some people going through drug withdrawal might feel bugs crawling on their skin, and a strange smell with no source can sometimes be an early sign of a seizure disorder.
                    </p>


                    <h2 id="clinical-definition">What is the clinical definition of confabulations?</h2>
                    <p>The DSM-5-TR does not provide a formal definition of confabulation. It is not listed as a standalone diagnostic criterion or disorder.
                      Confabulation is primarily a neuropsychological concept rather than a psychiatric diagnostic term, and it is more extensively discussed in clinical neuropsychology literature than in DSM manuals.
                      In neuropsychology, Schnider in 2005 defined confabulations as the unintentional fabrication of memories, typically to fill in gaps caused by amnesia.
                      Korsakoff's syndrome is a brain disorder often caused by long-term alcohol misuse, leading to severe memory problems. A person with this condition might firmly believe they had breakfast that morning at a favorite café, which actually closed years ago.
                      They are not trying to lie. Their brain is unknowingly filling in the blanks. This is known as confabulation. It is when someone creates false memories without realizing it, to make sense of gaps in their memory.
                    </p>

                    <h2 id="about-ai">What About AI?</h2>
                    <p>AI does not know anything in the human sense. What it does is predict. When a chatbot generates a false but plausible answer, it is not sensing a ghost or hearing voices that are not there.
                      It is predicting an output from an input based on patterns in its training data. It is trying to fill in what should come next, even if it makes it up, just as a confabulating brain fills in what must have happened.
                    </p>


                    <h2 id="call-it">Call It What It Is</h2>
                    <p>Using "hallucination" to describe AI errors encourages an idea of machines, as if they had inner worlds and private perceptions. "Confabulation" is more accurate and more useful. It reminds us these systems generate text, not truth as they do not know what is real in a human sense. They stitch together words based on statistical echoes, not reality checks.
                      Understanding this helps developers build better tools. Ones that flag uncertainty or backtrack when unsure. And it helps users stay clear-eyed: the machine is not dreaming or lying. It is guessing, with confidence but no conscience.
                      So let us drop the romance of "hallucination." AI does not perceive. It predicts. When it gets things wrong, it is not deluded. It is making things up. It is confabulating. And just as in medicine, proper treatment begins with an accurate diagnosis. Therefore, naming the problem correctly is the first step to addressing it.
                    </p>

                </div>
            </article>
        </main>
        
    </div>

    <script src="../script.js"></script>
</body>
</html>
